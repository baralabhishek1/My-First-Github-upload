{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOn1P4HKMRxMTGFoWNpyxE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baralabhishek1/My-First-Github-upload/blob/master/user_desc%2Bimage_conv2d_onwatsondata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSQe0DsgSLpq",
        "outputId": "e52a8041-a6c2-4286-f965-c3a2fb7684ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install os_sys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ahlGKcuS2M7",
        "outputId": "3f1dba92-359f-4a80-bfef-94223c320ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting os_sys\n",
            "  Downloading os_sys-2.1.4-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting pygubu (from os_sys)\n",
            "  Downloading pygubu-0.35.6-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from os_sys) (2024.2)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.10/dist-packages (from os_sys) (0.5.2)\n",
            "Collecting progress (from os_sys)\n",
            "  Downloading progress-1.6.tar.gz (7.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from os_sys) (4.66.6)\n",
            "Collecting progressbar (from os_sys)\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from os_sys) (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from os_sys) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from os_sys) (1.16.0)\n",
            "Collecting jupyter (from os_sys)\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from os_sys) (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from os_sys) (4.12.3)\n",
            "Collecting Eel (from os_sys)\n",
            "  Downloading eel-0.17.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting extract-zip (from os_sys)\n",
            "  Downloading extract_zip-1.0.0-py3-none-any.whl.metadata (403 bytes)\n",
            "INFO: pip is looking at multiple versions of os-sys to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting os_sys\n",
            "  Downloading os_sys-2.1.3-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Downloading os_sys-2.1.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Downloading os_sys-2.1.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Downloading os_sys-2.1.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Downloading os_sys-2.0.9-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Downloading os_sys-2.0.8-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Downloading os_sys-2.0.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "INFO: pip is still looking at multiple versions of os-sys to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading os_sys-2.0.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "  Downloading os_sys-2.0.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "  Downloading os_sys-2.0.4-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting webview (from os_sys)\n",
            "  Downloading webview-0.1.5.tar.gz (18 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install beautifulsoup4\n",
        "!pip install emoji\n",
        "!pip install pandas\n",
        "!pip install numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws2LaUbF2ax_",
        "outputId": "971fb227-ec40-42e4-b8ee-d0e2f42c3396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.0\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas scikit-learn vaderSentiment beautifulsoup4 nltk emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAmqxDoBUEgY",
        "outputId": "41b26921-455f-40af-d27d-4c655ce8d551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        "import emoji, string\n",
        "import itertools\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Update Keras imports to TensorFlow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import Dense, Input, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Conv1D, MaxPool1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed, concatenate, add, Conv2D, SpatialDropout1D, MaxPooling2D\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from tensorflow.keras import initializers\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import os, codecs\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
        "\n",
        "np.random.seed(100)\n"
      ],
      "metadata": {
        "id": "ForYCBr4S3LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pJA7ShSUhUo",
        "outputId": "24af4794-e669-4ffe-e3ce-f63d670a79bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01NAHOr0Ur4w",
        "outputId": "48ab07fc-f9dc-4bd0-f6bb-8eb17394a19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SENT_LENGTH = 100\n",
        "MAX_SENTS = 15\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.1\n",
        "TEST_SPLIT = 0.2\n"
      ],
      "metadata": {
        "id": "CAyjAuGIVSzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " DATA CLEANING"
      ],
      "metadata": {
        "id": "i0gIMz4mVbKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# emoticons\n",
        "def load_dict_smileys():\n",
        "\n",
        "    return {\n",
        "        \":‑)\":\"smiley\",\n",
        "        \":-]\":\"smiley\",\n",
        "        \":-3\":\"smiley\",\n",
        "        \":->\":\"smiley\",\n",
        "        \"8-)\":\"smiley\",\n",
        "        \":-}\":\"smiley\",\n",
        "        \":)\":\"smiley\",\n",
        "        \":]\":\"smiley\",\n",
        "        \":3\":\"smiley\",\n",
        "        \":>\":\"smiley\",\n",
        "        \"8)\":\"smiley\",\n",
        "        \":}\":\"smiley\",\n",
        "        \":o)\":\"smiley\",\n",
        "        \":c)\":\"smiley\",\n",
        "        \":^)\":\"smiley\",\n",
        "        \"=]\":\"smiley\",\n",
        "        \"=)\":\"smiley\",\n",
        "        \":-))\":\"smiley\",\n",
        "        \":‑D\":\"smiley\",\n",
        "        \"8‑D\":\"smiley\",\n",
        "        \"x‑D\":\"smiley\",\n",
        "        \"X‑D\":\"smiley\",\n",
        "        \":D\":\"smiley\",\n",
        "        \"8D\":\"smiley\",\n",
        "        \"xD\":\"smiley\",\n",
        "        \"XD\":\"smiley\",\n",
        "        \":‑(\":\"sad\",\n",
        "        \":‑c\":\"sad\",\n",
        "        \":‑<\":\"sad\",\n",
        "        \":‑[\":\"sad\",\n",
        "        \":(\":\"sad\",\n",
        "        \":c\":\"sad\",\n",
        "        \":<\":\"sad\",\n",
        "        \":[\":\"sad\",\n",
        "        \":-||\":\"sad\",\n",
        "        \">:[\":\"sad\",\n",
        "        \":{\":\"sad\",\n",
        "        \":@\":\"sad\",\n",
        "        \">:(\":\"sad\",\n",
        "        \":'‑(\":\"sad\",\n",
        "        \":'(\":\"sad\",\n",
        "        \":‑P\":\"playful\",\n",
        "        \"X‑P\":\"playful\",\n",
        "        \"x‑p\":\"playful\",\n",
        "        \":‑p\":\"playful\",\n",
        "        \":‑Þ\":\"playful\",\n",
        "        \":‑þ\":\"playful\",\n",
        "        \":‑b\":\"playful\",\n",
        "        \":P\":\"playful\",\n",
        "        \"XP\":\"playful\",\n",
        "        \"xp\":\"playful\",\n",
        "        \":p\":\"playful\",\n",
        "        \":Þ\":\"playful\",\n",
        "        \":þ\":\"playful\",\n",
        "        \":b\":\"playful\",\n",
        "        \"<3\":\"love\"\n",
        "        }\n",
        "\n",
        "# self defined contractions\n",
        "def load_dict_contractions():\n",
        "\n",
        "    return {\n",
        "        \"ain't\":\"is not\",\n",
        "        \"amn't\":\"am not\",\n",
        "        \"aren't\":\"are not\",\n",
        "        \"can't\":\"cannot\",\n",
        "        \"'cause\":\"because\",\n",
        "        \"couldn't\":\"could not\",\n",
        "        \"couldn't've\":\"could not have\",\n",
        "        \"could've\":\"could have\",\n",
        "        \"daren't\":\"dare not\",\n",
        "        \"daresn't\":\"dare not\",\n",
        "        \"dasn't\":\"dare not\",\n",
        "        \"didn't\":\"did not\",\n",
        "        \"doesn't\":\"does not\",\n",
        "        \"don't\":\"do not\",\n",
        "        \"e'er\":\"ever\",\n",
        "        \"em\":\"them\",\n",
        "        \"everyone's\":\"everyone is\",\n",
        "        \"finna\":\"fixing to\",\n",
        "        \"gimme\":\"give me\",\n",
        "        \"gonna\":\"going to\",\n",
        "        \"gon't\":\"go not\",\n",
        "        \"gotta\":\"got to\",\n",
        "        \"hadn't\":\"had not\",\n",
        "        \"hasn't\":\"has not\",\n",
        "        \"haven't\":\"have not\",\n",
        "        \"he'd\":\"he would\",\n",
        "        \"he'll\":\"he will\",\n",
        "        \"he's\":\"he is\",\n",
        "        \"he've\":\"he have\",\n",
        "        \"how'd\":\"how would\",\n",
        "        \"how'll\":\"how will\",\n",
        "        \"how're\":\"how are\",\n",
        "        \"how's\":\"how is\",\n",
        "        \"i'd\":\"i would\",\n",
        "        \"i'll\":\"i will\",\n",
        "        \"i'm\":\"i am\",\n",
        "        \"i'm'a\":\"i am about to\",\n",
        "        \"i'm'o\":\"i am going to\",\n",
        "        \"isn't\":\"is not\",\n",
        "        \"it'd\":\"it would\",\n",
        "        \"it'll\":\"it will\",\n",
        "        \"it's\":\"it is\",\n",
        "        \"i've\":\"i have\",\n",
        "        \"kinda\":\"kind of\",\n",
        "        \"let's\":\"let us\",\n",
        "        \"mayn't\":\"may not\",\n",
        "        \"may've\":\"may have\",\n",
        "        \"mightn't\":\"might not\",\n",
        "        \"might've\":\"might have\",\n",
        "        \"mustn't\":\"must not\",\n",
        "        \"mustn't've\":\"must not have\",\n",
        "        \"must've\":\"must have\",\n",
        "        \"needn't\":\"need not\",\n",
        "        \"ne'er\":\"never\",\n",
        "        \"o'\":\"of\",\n",
        "        \"o'er\":\"over\",\n",
        "        \"ol'\":\"old\",\n",
        "        \"oughtn't\":\"ought not\",\n",
        "        \"shalln't\":\"shall not\",\n",
        "        \"shan't\":\"shall not\",\n",
        "        \"she'd\":\"she would\",\n",
        "        \"she'll\":\"she will\",\n",
        "        \"she's\":\"she is\",\n",
        "        \"shouldn't\":\"should not\",\n",
        "        \"shouldn't've\":\"should not have\",\n",
        "        \"should've\":\"should have\",\n",
        "        \"somebody's\":\"somebody is\",\n",
        "        \"someone's\":\"someone is\",\n",
        "        \"something's\":\"something is\",\n",
        "        \"that'd\":\"that would\",\n",
        "        \"that'll\":\"that will\",\n",
        "        \"that're\":\"that are\",\n",
        "        \"that's\":\"that is\",\n",
        "        \"there'd\":\"there would\",\n",
        "        \"there'll\":\"there will\",\n",
        "        \"there're\":\"there are\",\n",
        "        \"there's\":\"there is\",\n",
        "        \"these're\":\"these are\",\n",
        "        \"they'd\":\"they would\",\n",
        "        \"they'll\":\"they will\",\n",
        "        \"they're\":\"they are\",\n",
        "        \"they've\":\"they have\",\n",
        "        \"this's\":\"this is\",\n",
        "        \"those're\":\"those are\",\n",
        "        \"'tis\":\"it is\",\n",
        "        \"'twas\":\"it was\",\n",
        "        \"wanna\":\"want to\",\n",
        "        \"wasn't\":\"was not\",\n",
        "        \"we'd\":\"we would\",\n",
        "        \"we'd've\":\"we would have\",\n",
        "        \"we'll\":\"we will\",\n",
        "        \"we're\":\"we are\",\n",
        "        \"weren't\":\"were not\",\n",
        "        \"we've\":\"we have\",\n",
        "        \"what'd\":\"what did\",\n",
        "        \"what'll\":\"what will\",\n",
        "        \"what're\":\"what are\",\n",
        "        \"what's\":\"what is\",\n",
        "        \"what've\":\"what have\",\n",
        "        \"when's\":\"when is\",\n",
        "        \"where'd\":\"where did\",\n",
        "        \"where're\":\"where are\",\n",
        "        \"where's\":\"where is\",\n",
        "        \"where've\":\"where have\",\n",
        "        \"which's\":\"which is\",\n",
        "        \"who'd\":\"who would\",\n",
        "        \"who'd've\":\"who would have\",\n",
        "        \"who'll\":\"who will\",\n",
        "        \"who're\":\"who are\",\n",
        "        \"who's\":\"who is\",\n",
        "        \"who've\":\"who have\",\n",
        "        \"why'd\":\"why did\",\n",
        "        \"why're\":\"why are\",\n",
        "        \"why's\":\"why is\",\n",
        "        \"won't\":\"will not\",\n",
        "        \"wouldn't\":\"would not\",\n",
        "        \"would've\":\"would have\",\n",
        "        \"y'all\":\"you all\",\n",
        "        \"you'd\":\"you would\",\n",
        "        \"you'll\":\"you will\",\n",
        "        \"you're\":\"you are\",\n",
        "        \"you've\":\"you have\",\n",
        "        \"whatcha\":\"what are you\",\n",
        "        \"luv\":\"love\",\n",
        "        \"sux\":\"sucks\"\n",
        "        }\n",
        "\n",
        "def reduce_lengthening(text):\n",
        "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
        "    return pattern.sub(r\"\\1\\1\", text)\n",
        "\n",
        "def clean_str(string):\n",
        "\n",
        "    string = re.sub(r'http\\S+', '', string, flags=re.MULTILINE)\n",
        "    string = re.sub(r'www.\\S+', '', string, flags=re.MULTILINE)\n",
        "#    string = re.sub(r'@\\S+', '', string, flags=re.MULTILINE)\n",
        "    string = re.sub(r\"http\\S+\", \"\", string, flags=re.MULTILINE)\n",
        "#    print(string)\n",
        "    string = string.replace('\\n', ' ').replace('\\t', ' ')\n",
        "    string = string.lower()\n",
        "    string = reduce_lengthening(string)\n",
        "\n",
        "    #CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n",
        "    CONTRACTIONS = load_dict_contractions()\n",
        "    string = string.replace(\"’\",\"'\").replace(\"“\",\"'\").replace(\"”\",\"'\").replace(\"’\",\"'\").replace(\"‘\",\"'\")\n",
        "    words = string.split()\n",
        "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
        "    string = \" \".join(reformed)\n",
        "\n",
        "    #Deal with emoticons source: https://en.wikipedia.org/wiki/List_of_emoticons\n",
        "    SMILEY = load_dict_smileys()\n",
        "    words = string.split()\n",
        "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
        "    string = \" \".join(reformed)\n",
        "\n",
        "    #Deal with emojis\n",
        "    string = emoji.demojize(string)\n",
        "\n",
        "    string = ' '.join(string.split())\n",
        "\n",
        "    string = re.sub('[^A-Za-z0-9.?;!]+', ' ', string).lstrip().lower()\n",
        "    string = string.replace(\";\",\" ; \").replace(\".\",\" . \").replace(\"?\",\" ? \").replace(\"!\",\" ! \")\n",
        "\n",
        "    string = ' '.join(string.split())\n",
        "\n",
        "#    load()\n",
        "#    segs = segment(string)\n",
        "#    print(segs)\n",
        "#    string = ' '.join(segs)\n",
        "#    print(string)\n",
        "\n",
        "    return string.strip().lower()"
      ],
      "metadata": {
        "id": "uM3DoZnqVd1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention Layer Code"
      ],
      "metadata": {
        "id": "wn2elyr6Vtib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Layer\n",
        "from keras import initializers\n",
        "\n",
        "class AttLayer(Layer):\n",
        "    def __init__(self, attention_dim):\n",
        "        super(AttLayer, self).__init__()\n",
        "        self.attention_dim = attention_dim\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Check that input shape has 3 dimensions (batch_size, timesteps, features)\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        # Initialize the weights using add_weight, which Keras can manage\n",
        "        self.W = self.add_weight(name='W',\n",
        "                                 shape=(input_shape[-1], self.attention_dim),\n",
        "                                 initializer='normal',\n",
        "                                 trainable=True)\n",
        "        self.b = self.add_weight(name='b',\n",
        "                                 shape=(self.attention_dim,),\n",
        "                                 initializer='normal',\n",
        "                                 trainable=True)\n",
        "        self.u = self.add_weight(name='u',\n",
        "                                 shape=(self.attention_dim, 1),\n",
        "                                 initializer='normal',\n",
        "                                 trainable=True)\n",
        "\n",
        "        super(AttLayer, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        # Mask is not used in this attention mechanism, so return None\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        # Compute the attention scores\n",
        "        uit = tf.tanh(tf.keras.backend.bias_add(tf.matmul(x, self.W), self.b))  # Shape: [batch_size, timesteps, attention_dim]\n",
        "        ait = tf.matmul(uit, self.u)  # Shape: [batch_size, timesteps, 1]\n",
        "        ait = tf.squeeze(ait, -1)  # Shape: [batch_size, timesteps]\n",
        "\n",
        "        ait = tf.exp(ait)  # Exponentiate to get attention weights\n",
        "\n",
        "        if mask is not None:\n",
        "            ait *= tf.cast(mask, tf.float32)  # Apply the mask\n",
        "\n",
        "        # Normalize the attention scores\n",
        "        ait /= tf.cast(tf.reduce_sum(ait, axis=1, keepdims=True) + K.epsilon(), tf.float32)\n",
        "\n",
        "        # Compute the weighted input and return the output\n",
        "        ait = tf.expand_dims(ait, axis=-1)  # Shape: [batch_size, timesteps, 1]\n",
        "        weighted_input = x * ait  # Apply attention weights\n",
        "        output = tf.reduce_sum(weighted_input, axis=1)  # Shape: [batch_size, features]\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # The output shape will be [batch_size, features] (same number of features as input)\n",
        "        return (input_shape[0], input_shape[-1])\n"
      ],
      "metadata": {
        "id": "4nQw1X_aVwEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the Dataset file to use in future"
      ],
      "metadata": {
        "id": "uw1Rb0T3YHUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "zip_file = '/content/drive/MyDrive/Dataset.zip'\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/drive/MyDrive/')\n",
        "\n"
      ],
      "metadata": {
        "id": "PWQVhzH2XACB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from skimage.transform import resize\n",
        "import numpy\n",
        "prof_feat = np.load('/content/drive/MyDrive/profile_feats.npy', allow_pickle=True)\n",
        "\n",
        "print(prof_feat.shape)\n",
        "\n",
        "for i in range(len(prof_feat)):\n",
        "\tprof_feat[i] = resize(prof_feat[i], (48, 48, 3)).astype(int)\n",
        "\n",
        "features = []\n",
        "\n",
        "for i in range(len(prof_feat)):\n",
        "    features.append(prof_feat[i])\n",
        "\n",
        "features = np.array(features)\n",
        "print(features.shape)\n",
        "\n",
        "nRows,nCols,nDims = features.shape[1:]\n",
        "features = features.reshape(features.shape[0], nRows, nCols, nDims)\n",
        "input_shape = (nRows, nCols, nDims)\n",
        "\n",
        "pos_df = pd.read_csv('/content/drive/MyDrive/Dataset/pos_tweets.csv', sep=',', lineterminator='\\n')\n",
        "neg_df = pd.read_csv('/content/drive/MyDrive/Dataset/neg_tweets.csv', sep=',', lineterminator='\\n')\n",
        "\n",
        "print(pos_df)\n",
        "print(neg_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_bkReVJVxih",
        "outputId": "cf0d3c00-560c-4388-c06a-28b4a5dc5b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10603,)\n",
            "(10603, 48, 48, 3)\n",
            "                                       user_description\n",
            "0                                               alright\n",
            "1     ADHD & Diverse-Ability Transition Coach/Traine...\n",
            "2     queer trans bot | she/her | property of @Corde...\n",
            "3                    People tend to say I'm stone cold.\n",
            "4     Hi, I'm Kris. I'm an okay cross/cosplayer. he/...\n",
            "...                                                 ...\n",
            "5887  I like lots of groups and I watch dramas. 17. ...\n",
            "5888                           live and let live•WSU•♎️\n",
            "5889                       I love Robbie Kay and music!\n",
            "5890  A Northern Star! \\n\\nThe latest News! Tips and...\n",
            "5891                                          dog luver\n",
            "\n",
            "[5892 rows x 1 columns]\n",
            "                                       user_description\n",
            "0     Food addict, sweet tooth, house music and tequ...\n",
            "1     Bollywood and Celebrities Brands Breaking News...\n",
            "2             ‏‏‏حساب شخصي. مدمن كرة قدم. CFC #Ittihad#\n",
            "3                                      IG: _RichyRozay_\n",
            "4                                       fuck em we ball\n",
            "...                                                 ...\n",
            "4702  ‏‏‏‏‏‏.\\n.\\nلا تحرص على كثرة الأصحاب والمعارف ...\n",
            "4703  YouTuber & Twitch Streamer. Just a dude with a...\n",
            "4704                      Forbes reserved space for me.\n",
            "4705  Future SoftwareDeveloper|| #Chelsea || Passion...\n",
            "4706                              Apparently i eat ass.\n",
            "\n",
            "[4707 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_df = pos_df.dropna(subset=['user_description'])\n",
        "neg_df = neg_df.dropna(subset=['user_description'])\n",
        "\n",
        "pos_df['labels'] = 0\n",
        "neg_df['labels'] = 1\n",
        "\n",
        "print(pos_df)\n",
        "print(neg_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsCqsu_nWYRv",
        "outputId": "a71f03d9-e96c-4492-aca6-d1ead8da7741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       user_description  labels\n",
            "0                                               alright       0\n",
            "1     ADHD & Diverse-Ability Transition Coach/Traine...       0\n",
            "2     queer trans bot | she/her | property of @Corde...       0\n",
            "3                    People tend to say I'm stone cold.       0\n",
            "4     Hi, I'm Kris. I'm an okay cross/cosplayer. he/...       0\n",
            "...                                                 ...     ...\n",
            "5887  I like lots of groups and I watch dramas. 17. ...       0\n",
            "5888                           live and let live•WSU•♎️       0\n",
            "5889                       I love Robbie Kay and music!       0\n",
            "5890  A Northern Star! \\n\\nThe latest News! Tips and...       0\n",
            "5891                                          dog luver       0\n",
            "\n",
            "[5892 rows x 2 columns]\n",
            "                                       user_description  labels\n",
            "0     Food addict, sweet tooth, house music and tequ...       1\n",
            "1     Bollywood and Celebrities Brands Breaking News...       1\n",
            "2             ‏‏‏حساب شخصي. مدمن كرة قدم. CFC #Ittihad#       1\n",
            "3                                      IG: _RichyRozay_       1\n",
            "4                                       fuck em we ball       1\n",
            "...                                                 ...     ...\n",
            "4702  ‏‏‏‏‏‏.\\n.\\nلا تحرص على كثرة الأصحاب والمعارف ...       1\n",
            "4703  YouTuber & Twitch Streamer. Just a dude with a...       1\n",
            "4704                      Forbes reserved space for me.       1\n",
            "4705  Future SoftwareDeveloper|| #Chelsea || Passion...       1\n",
            "4706                              Apparently i eat ass.       1\n",
            "\n",
            "[4707 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = [pos_df, neg_df]\n",
        "\n",
        "train = pd.concat(df)\n",
        "\n",
        "print(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2PglDkZYkpf",
        "outputId": "4548f45f-5824-4a39-e020-45c164bf91e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       user_description  labels\n",
            "0                                               alright       0\n",
            "1     ADHD & Diverse-Ability Transition Coach/Traine...       0\n",
            "2     queer trans bot | she/her | property of @Corde...       0\n",
            "3                    People tend to say I'm stone cold.       0\n",
            "4     Hi, I'm Kris. I'm an okay cross/cosplayer. he/...       0\n",
            "...                                                 ...     ...\n",
            "4702  ‏‏‏‏‏‏.\\n.\\nلا تحرص على كثرة الأصحاب والمعارف ...       1\n",
            "4703  YouTuber & Twitch Streamer. Just a dude with a...       1\n",
            "4704                      Forbes reserved space for me.       1\n",
            "4705  Future SoftwareDeveloper|| #Chelsea || Passion...       1\n",
            "4706                              Apparently i eat ass.       1\n",
            "\n",
            "[10599 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_df['labels'] = 0\n",
        "neg_df['labels'] = 1\n",
        "print(pos_df)\n",
        "print(neg_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxVeB9Byv6DM",
        "outputId": "89e554b8-b19f-49fe-e5f3-7746af4aa852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       user_description  labels\n",
            "0                                               alright       0\n",
            "1     ADHD & Diverse-Ability Transition Coach/Traine...       0\n",
            "2     queer trans bot | she/her | property of @Corde...       0\n",
            "3                    People tend to say I'm stone cold.       0\n",
            "4     Hi, I'm Kris. I'm an okay cross/cosplayer. he/...       0\n",
            "...                                                 ...     ...\n",
            "5887  I like lots of groups and I watch dramas. 17. ...       0\n",
            "5888                           live and let live•WSU•♎️       0\n",
            "5889                       I love Robbie Kay and music!       0\n",
            "5890  A Northern Star! \\n\\nThe latest News! Tips and...       0\n",
            "5891                                          dog luver       0\n",
            "\n",
            "[5892 rows x 2 columns]\n",
            "                                       user_description  labels\n",
            "0     Food addict, sweet tooth, house music and tequ...       1\n",
            "1     Bollywood and Celebrities Brands Breaking News...       1\n",
            "2             ‏‏‏حساب شخصي. مدمن كرة قدم. CFC #Ittihad#       1\n",
            "3                                      IG: _RichyRozay_       1\n",
            "4                                       fuck em we ball       1\n",
            "...                                                 ...     ...\n",
            "4702  ‏‏‏‏‏‏.\\n.\\nلا تحرص على كثرة الأصحاب والمعارف ...       1\n",
            "4703  YouTuber & Twitch Streamer. Just a dude with a...       1\n",
            "4704                      Forbes reserved space for me.       1\n",
            "4705  Future SoftwareDeveloper|| #Chelsea || Passion...       1\n",
            "4706                              Apparently i eat ass.       1\n",
            "\n",
            "[4707 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = [pos_df, neg_df]\n",
        "\n",
        "train = pd.concat(df)\n",
        "\n",
        "print(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS8xm1KqwnMn",
        "outputId": "e448c89d-7ba9-4112-ead1-3509caefcc27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       user_description  labels\n",
            "0                                               alright       0\n",
            "1     ADHD & Diverse-Ability Transition Coach/Traine...       0\n",
            "2     queer trans bot | she/her | property of @Corde...       0\n",
            "3                    People tend to say I'm stone cold.       0\n",
            "4     Hi, I'm Kris. I'm an okay cross/cosplayer. he/...       0\n",
            "...                                                 ...     ...\n",
            "4702  ‏‏‏‏‏‏.\\n.\\nلا تحرص على كثرة الأصحاب والمعارف ...       1\n",
            "4703  YouTuber & Twitch Streamer. Just a dude with a...       1\n",
            "4704                      Forbes reserved space for me.       1\n",
            "4705  Future SoftwareDeveloper|| #Chelsea || Passion...       1\n",
            "4706                              Apparently i eat ass.       1\n",
            "\n",
            "[10599 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.reset_index()"
      ],
      "metadata": {
        "id": "61SjxOuWwq3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install ekphrasis\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "\n",
        "    # corpus from which the word statistics are going to be used\n",
        "    # for word segmentation\n",
        "    segmenter=\"twitter\",\n",
        "\n",
        "    # corpus from which the word statistics are going to be used\n",
        "    # for spell correction\n",
        "    corrector=\"twitter\",\n",
        "\n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=True,  # spell correction for elongated words\n",
        "\n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "\n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSeoZ3dHYorQ",
        "outputId": "5690694f-7e51-4be9-a000-1d243cc6f959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ekphrasis\n",
            "  Downloading ekphrasis-0.5.4-py3-none-any.whl.metadata (610 bytes)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (2.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (4.66.6)\n",
            "Collecting colorama (from ekphrasis)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting ujson (from ekphrasis)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.8.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.9.1)\n",
            "Collecting ftfy (from ekphrasis)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (1.26.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->ekphrasis) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (2024.9.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->ekphrasis) (1.16.0)\n",
            "Downloading ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ujson, ftfy, colorama, ekphrasis\n",
            "Successfully installed colorama-0.4.6 ekphrasis-0.5.4 ftfy-6.3.1 ujson-5.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n",
            "Reading twitter - 1grams ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut1hJYCdZGtf",
        "outputId": "3899cb9e-9301-43a4-ca4c-c08e6ba4ebe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       index                                   user_description  labels\n",
            "0          0                                            alright       0\n",
            "1          1  ADHD & Diverse-Ability Transition Coach/Traine...       0\n",
            "2          2  queer trans bot | she/her | property of @Corde...       0\n",
            "3          3                 People tend to say I'm stone cold.       0\n",
            "4          4  Hi, I'm Kris. I'm an okay cross/cosplayer. he/...       0\n",
            "...      ...                                                ...     ...\n",
            "10594   4702  ‏‏‏‏‏‏.\\n.\\nلا تحرص على كثرة الأصحاب والمعارف ...       1\n",
            "10595   4703  YouTuber & Twitch Streamer. Just a dude with a...       1\n",
            "10596   4704                      Forbes reserved space for me.       1\n",
            "10597   4705  Future SoftwareDeveloper|| #Chelsea || Passion...       1\n",
            "10598   4706                              Apparently i eat ass.       1\n",
            "\n",
            "[10599 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize\n",
        "\n",
        "textonly = []; descronly = []; labels = []\n",
        "\n",
        "for idx in range(train.user_description.shape[0]):\n",
        "\n",
        "    text = \" \".join(text_processor.pre_process_doc(train.user_description[idx]))\n",
        "    textonly.append(text)\n",
        "\n",
        "    descr = \" \".join(text_processor.pre_process_doc(train.user_description[idx]))\n",
        "    descronly.append(descr)\n",
        "    labels.append(train.labels[idx])\n",
        "\n",
        "textonly = descronly"
      ],
      "metadata": {
        "id": "60msPdeiYtEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.columns)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNijGHC9spoP",
        "outputId": "e214301f-a482-4262-e2e0-ca6d0c435e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['index', 'user_description', 'labels'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# At this point, textonly and descronly should contain the preprocessed data\n",
        "# You can check them by printing some samples\n",
        "print(textonly[:5])\n",
        "print(descronly[:5])\n",
        "print(labels[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njt2K65CeaH7",
        "outputId": "87afe376-e846-44e6-bdf5-916bca12f8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alright', '<allcaps> adhd </allcaps> & diverse - ability transition coach / trainer - post - secondary students , career exploration & entrepreneurs . <number> <allcaps> ednns </allcaps> entrepreneur of the year . <hashtag> str ad degies </hashtag>', 'queer trans bot | she / her | property of <user>', 'people tend to say i am stone cold .', 'hi , i am kris . i am an okay cross / cosplayer . he / they , trans masc / genderqueer . thank you !']\n",
            "['alright', '<allcaps> adhd </allcaps> & diverse - ability transition coach / trainer - post - secondary students , career exploration & entrepreneurs . <number> <allcaps> ednns </allcaps> entrepreneur of the year . <hashtag> str ad degies </hashtag>', 'queer trans bot | she / her | property of <user>', 'people tend to say i am stone cold .', 'hi , i am kris . i am an okay cross / cosplayer . he / they , trans masc / genderqueer . thank you !']\n",
            "[0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(textonly)\n",
        "\n",
        "textdata = np.zeros((len(textonly), MAX_SENT_LENGTH), dtype='int32')\n",
        "#descrdata = np.zeros((len(descronly), MAX_SENT_LENGTH), dtype='int32')\n",
        "\n",
        "for i, sent in enumerate(textonly):\n",
        "    wordTokens = text_to_word_sequence(sent)\n",
        "    k = 0\n",
        "    for _, word in enumerate(wordTokens):\n",
        "        if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
        "            textdata[i, k] = tokenizer.word_index[word]\n",
        "            k = k + 1\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('\\nTotal %s unique tokens.' % len(word_index))\n",
        "\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "print('\\nShape of data tensor:', textdata.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "metadata": {
        "id": "ks6t5qxqxP1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86e7285-8d9c-4ab3-8345-9cb3ebd9faa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total 24388 unique tokens.\n",
            "\n",
            "Shape of data tensor: (10599, 100)\n",
            "Shape of label tensor: (10599, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(textdata))  # Length of textdata\n",
        "print(len(labels))     # Length of labels\n",
        "print(len(features))   # Length of features\n",
        "print(len(textonly))   # Length of textonly\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNKBJVD7udBN",
        "outputId": "d0217e56-2f0f-49fd-aaa8-a91de616e519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10599\n",
            "10599\n",
            "10603\n",
            "10599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_len = min(len(textdata), len(labels), len(features), len(textonly))\n",
        "textdata = textdata[:min_len]\n",
        "labels = labels[:min_len]\n",
        "features = features[:min_len]\n",
        "textonly = textonly[:min_len]\n"
      ],
      "metadata": {
        "id": "899-DtX5utbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test, prof_feat_train, prof_feat_test, train_text, test_text = train_test_split(textdata, labels, features, textonly, test_size=0.2, random_state=42, stratify = labels)\n",
        "\n",
        "\n",
        "print(len(x_test), len(test_text))\n",
        "print(test_text[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew-wx0-MZ--o",
        "outputId": "de4de737-f1a4-41b5-bede-8bafa65ea82d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2120 2120\n",
            "['secondary creation werewolf germany from hetalia bot . return follows are automatic . please check out the manual ! [ botmaster : <user> | icon : official art ]', 'make my own jewellery / cards listen to dayshell / <allcaps> ptv </allcaps> / <allcaps> sws </allcaps> / young guns / <allcaps> bmth </allcaps> / <allcaps> atl </allcaps> / <allcaps> bfmv </allcaps> and <allcaps> omm </allcaps> . <repeated> ect also a caring person', 'i belong to <user> / <hashtag> hillbilly princess </hashtag> / kinda crazy / <hashtag> jiz </hashtag> / ig : twilitwolfy', '사랑해', '1 0 k a show . dm me for booking']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLoading GloVe model, this can take some time...\\n...\")\n",
        "embeddings_index = {}\n",
        "f = open('/content/drive/MyDrive/glove.6B.300d.txt', encoding='utf-8')\n",
        "#f = open('word2vec_cdotdata.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "        coefs = np.asarray(values[1:], dtype='float')\n",
        "        embeddings_index[word] = coefs\n",
        "    except ValueError:\n",
        "        continue\n",
        "f.close()\n",
        "print(\"Completed loading pretrained models.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdE2U_fkaGta",
        "outputId": "18884774-1267-41df-d401-1a52448b1e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading GloVe model, this can take some time...\n",
            "...\n",
            "Completed loading pretrained models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nTotal %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj4qSGpBaNC2",
        "outputId": "643c7b00-28dd-4a79-baaf-65060d1d98db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## building Hierachical Attention network"
      ],
      "metadata": {
        "id": "zPpyzVEyv1VV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.random.random((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "target_names_dep = ['dep', 'non-dep']\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            300,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SENT_LENGTH,\n",
        "                            trainable=False,\n",
        "                            mask_zero=True)\n",
        "\n",
        "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
        "descr_input = Input(shape=input_shape, dtype='float32')\n",
        "\n",
        "conv = Conv2D(5, (2, 2), padding='same', activation='relu', input_shape=input_shape)(descr_input)\n",
        "conv = MaxPooling2D(pool_size=(3, 3))(conv)\n",
        "conv = Conv2D(5, (2, 2), activation='relu')(conv)\n",
        "conv = MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "conv = Dropout(0.25)(conv)\n",
        "\n",
        "conv = Flatten()(conv)\n",
        "\n",
        "conv = Dense(100, activation='relu')(conv)\n",
        "conv = Dropout(0.25)(conv)\n",
        "\n",
        "embedded_sequences = embedding_layer(sentence_input)\n",
        "\n",
        "l_gru = Bidirectional(GRU(128, return_sequences=True))(embedded_sequences)\n",
        "l_att_sent1 = AttLayer(100)(l_gru)\n",
        "l_att_sent1 = Dropout(0.25)(l_att_sent1)\n",
        "\n",
        "l_att_sent1 = Dense(100, activation='relu')(l_att_sent1)\n",
        "l_att_sent1 = Dropout(0.25)(l_att_sent1)\n",
        "\n",
        "out_emo = Dense(100, activation='relu')(concatenate([l_att_sent1, conv], axis=-1))\n",
        "out_emo = Dropout(0.25)(out_emo)\n",
        "\n",
        "out_emo = Dense(2, activation='softmax', name='out_emo')(out_emo)\n",
        "\n",
        "model = Model([sentence_input, descr_input], out_emo)\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "                   loss = {'out_emo':'categorical_crossentropy'},\n",
        "                   metrics = {'out_emo':'accuracy'})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_tlF5HsvuF1",
        "outputId": "dd3ed4b3-69fa-4399-b188-f0519b946fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight_file = 'Weights/user_desc+image_conv2d.keras'\n",
        "callback = [ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "\n",
        "\n",
        "model.fit([x_train, prof_feat_train], y_train, batch_size=64, epochs=15, shuffle=True, validation_split=0.1, verbose=1, callbacks=callback)\n",
        "model.load_weights(weight_file)\n",
        "model.save('models/user_desc+image_conv2d.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "rQpWW-4yvuCq",
        "outputId": "95956371-d21f-4a2c-d7ed-069bcdc6e671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 724ms/step - accuracy: 0.5998 - loss: 0.6617 - val_accuracy: 0.6521 - val_loss: 0.6078\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_acc available, skipping.\n",
            "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 728ms/step - accuracy: 0.6660 - loss: 0.6060 - val_accuracy: 0.6616 - val_loss: 0.5984\n",
            "Epoch 3/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 735ms/step - accuracy: 0.6858 - loss: 0.5796 - val_accuracy: 0.6616 - val_loss: 0.6089\n",
            "Epoch 4/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 736ms/step - accuracy: 0.7102 - loss: 0.5441 - val_accuracy: 0.6733 - val_loss: 0.6138\n",
            "Epoch 5/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 708ms/step - accuracy: 0.7348 - loss: 0.5170 - val_accuracy: 0.6816 - val_loss: 0.6185\n",
            "Epoch 6/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 712ms/step - accuracy: 0.7599 - loss: 0.4725 - val_accuracy: 0.6333 - val_loss: 0.6493\n",
            "Epoch 7/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 718ms/step - accuracy: 0.7898 - loss: 0.4312 - val_accuracy: 0.6663 - val_loss: 0.6869\n",
            "Epoch 8/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 715ms/step - accuracy: 0.8096 - loss: 0.3881 - val_accuracy: 0.6651 - val_loss: 0.7152\n",
            "Epoch 9/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 769ms/step - accuracy: 0.8416 - loss: 0.3377 - val_accuracy: 0.6639 - val_loss: 0.7475\n",
            "Epoch 10/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 715ms/step - accuracy: 0.8700 - loss: 0.2904 - val_accuracy: 0.6769 - val_loss: 0.8850\n",
            "Epoch 11/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 710ms/step - accuracy: 0.8923 - loss: 0.2398 - val_accuracy: 0.6722 - val_loss: 0.9973\n",
            "Epoch 12/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 751ms/step - accuracy: 0.9061 - loss: 0.2196 - val_accuracy: 0.6403 - val_loss: 1.0736\n",
            "Epoch 13/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 702ms/step - accuracy: 0.9273 - loss: 0.1755 - val_accuracy: 0.6427 - val_loss: 1.2023\n",
            "Epoch 14/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 724ms/step - accuracy: 0.9348 - loss: 0.1663 - val_accuracy: 0.6722 - val_loss: 1.2211\n",
            "Epoch 15/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 698ms/step - accuracy: 0.9470 - loss: 0.1386 - val_accuracy: 0.6663 - val_loss: 1.1664\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Weights/user_desc+image_conv2d.keras'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-52e2891ba23d>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprof_feat_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/user_desc+image_conv2d.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Weights/user_desc+image_conv2d.keras'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3JMfSLY1vt_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5ldvGAXvt8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hn_hZNsuvt5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l55aLmrcvt0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EFnb4-u7vtwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7FN7GnFtvtrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tpSKsd4Mvtjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FLGyFdEJvtVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the lengths of all variables involved in train_test_split\n",
        "print(len(textdata))        # Length of textdata\n",
        "print(len(labels))          # Length of labels\n",
        "print(len(sadness))         # Length of sadness\n",
        "print(len(joy))             # Length of joy\n",
        "print(len(fear))            # Length of fear\n",
        "print(len(disgust))         # Length of disgust\n",
        "print(len(anger))           # Length of anger\n",
        "print(len(sentiment))       # Length of sentiment\n",
        "print(len(features))        # Length of features\n",
        "print(len(cat1))            # Length of cat1\n",
        "print(len(cat2))            # Length of cat2\n",
        "print(len(cat3))            # Length of cat3\n",
        "print(len(textonly))        # Length of textonly\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGA0EY-Ea-fq",
        "outputId": "b1f1bcc2-1947-45dd-fc58-558022177182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10599\n",
            "10599\n",
            "10603\n",
            "10603\n",
            "10603\n",
            "10603\n",
            "10603\n",
            "10603\n",
            "10603\n",
            "10599\n",
            "10599\n",
            "10599\n",
            "10599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first few rows of the train DataFrame\n",
        "print(train.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1IYIJNFbkq9",
        "outputId": "68e49516-4147-4ba8-dd5b-ba882fd10275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   index                                   user_description sentiment  labels\n",
            "0      0                                            alright  positive       0\n",
            "1      1  ADHD & Diverse-Ability Transition Coach/Traine...  positive       0\n",
            "2      2  queer trans bot | she/her | property of @Corde...  positive       0\n",
            "3      3                 People tend to say I'm stone cold.  positive       0\n",
            "4      4  Hi, I'm Kris. I'm an okay cross/cosplayer. he/...  positive       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNtvWq5ecwt7",
        "outputId": "36b3a672-f9a7-486b-d967-5af1186d64bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   index                                   user_description sentiment  labels\n",
            "0      0                                            alright  positive       0\n",
            "1      1  ADHD & Diverse-Ability Transition Coach/Traine...  positive       0\n",
            "2      2  queer trans bot | she/her | property of @Corde...  positive       0\n",
            "3      3                 People tend to say I'm stone cold.  positive       0\n",
            "4      4  Hi, I'm Kris. I'm an okay cross/cosplayer. he/...  positive       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(textdata))\n",
        "print(len(labels))\n",
        "print(len(sadness))\n",
        "print(len(joy))\n",
        "# ... check other variables similarly\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89f4p8TGdVH0",
        "outputId": "6ba36529-8cd2-4a60-83ab-d2faf5535c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10599\n",
            "10599\n",
            "10603\n",
            "10603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_length = min(len(textdata), len(labels), len(sadness), len(joy), len(fear), len(disgust), len(anger), len(sentiment), len(features), len(cat1), len(cat2), len(cat3), len(textonly))\n",
        "textdata = textdata[:min_length]\n",
        "labels = labels[:min_length]\n",
        "sadness = sadness[:min_length]\n",
        "joy = joy[:min_length]\n",
        "fear = fear[:min_length]\n",
        "disgust = disgust[:min_length]\n",
        "anger = anger[:min_length]\n",
        "sentiment = sentiment[:min_length]\n",
        "features = features[:min_length]\n",
        "cat1 = cat1[:min_length]\n",
        "cat2 = cat2[:min_length]\n",
        "cat3 = cat3[:min_length]\n",
        "textonly = textonly[:min_length]"
      ],
      "metadata": {
        "id": "p8MIJiNOdhNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of textdata:\", len(textdata))\n",
        "print(\"Length of labels:\", len(labels))\n",
        "print(\"Length of sadness:\", len(sadness))\n",
        "print(\"Length of joy:\", len(joy))\n",
        "print(\"Length of fear:\", len(fear))\n",
        "print(\"Length of disgust:\", len(disgust))\n",
        "print(\"Length of anger:\", len(anger))\n",
        "print(\"Length of sentiment:\", len(sentiment))\n",
        "print(\"Length of features:\", len(features))\n",
        "print(\"Length of cat1:\", len(cat1))\n",
        "print(\"Length of cat2:\", len(cat2))\n",
        "print(\"Length of cat3:\", len(cat3))\n",
        "print(\"Length of textonly:\", len(textonly))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rq499dzdj_x",
        "outputId": "65fef524-4e89-4c22-95b1-8c811d7edc8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of textdata: 10599\n",
            "Length of labels: 10599\n",
            "Length of sadness: 10599\n",
            "Length of joy: 10599\n",
            "Length of fear: 10599\n",
            "Length of disgust: 10599\n",
            "Length of anger: 10599\n",
            "Length of sentiment: 10599\n",
            "Length of features: 10599\n",
            "Length of cat1: 10599\n",
            "Length of cat2: 10599\n",
            "Length of cat3: 10599\n",
            "Length of textonly: 10599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Ensure all inputs are numpy arrays or lists with the same length\n",
        "x_train, x_test, y_train, y_test, sad_train, sad_test, joy_train, joy_test, fear_train, fear_test, dis_train, dis_test, ang_train, ang_test, sent_train, sent_test, prof_feat_train, prof_feat_test, cat1_train, cat1_test, cat2_train, cat2_test, cat3_train, cat3_test, train_text, test_text = train_test_split(\n",
        "    textdata,\n",
        "    labels,\n",
        "    sadness, joy, fear, disgust, anger, sentiment, features, cat1, cat2, cat3, textonly,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "# Print the length of test data and some test text examples to confirm\n",
        "print(len(x_test), len(test_text))\n",
        "print(test_text[:5])  # Show the first 5 test text samples\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvmstpXlaTlk",
        "outputId": "9b2c27d9-add2-46f7-eca1-78a4611b9bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2120 2120\n",
            "['<hashtag> poetry </hashtag> <hashtag> micro poetry </hashtag> shortlisted for atlantis short story <number> . inspired by my love of <hashtag> words </hashtag> <hashtag> nature </hashtag> . volunteer facilitator with <user>', '<date> ❤️ sc : lexi_sheridan', 'pirate . princess . psych student . dancer . irn bru junkie . im right here overthinking .', 'this is <allcaps> smak penabur </allcaps> gading serpong official twitter account . we invite students , alumni , parents , teachers to follow our account to get the updated news', '<hashtag> anw 10 </hashtag> <hashtag> seahawk nation </hashtag> <number> / <number> of <hashtag> melgan </hashtag> <url>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sad_train = np.array(sad_train)\n",
        "sad_test = np.array(sad_test)\n",
        "sad_train = np.nan_to_num(sad_train)\n",
        "sad_test = np.nan_to_num(sad_test)\n",
        "\n",
        "joy_train = np.array(joy_train)\n",
        "joy_test = np.array(joy_test)\n",
        "joy_train = np.nan_to_num(joy_train)\n",
        "joy_test = np.nan_to_num(joy_test)\n",
        "\n",
        "fear_train = np.array(fear_train)\n",
        "fear_test = np.array(fear_test)\n",
        "fear_train = np.nan_to_num(fear_train)\n",
        "fear_test = np.nan_to_num(fear_test)\n",
        "\n",
        "dis_train = np.array(dis_train)\n",
        "dis_test = np.array(dis_test)\n",
        "dis_train = np.nan_to_num(dis_train)\n",
        "dis_test = np.nan_to_num(dis_test)\n",
        "\n",
        "ang_train = np.array(ang_train)\n",
        "ang_test = np.array(ang_test)\n",
        "ang_train = np.nan_to_num(ang_train)\n",
        "ang_test = np.nan_to_num(ang_test)\n",
        "\n",
        "sent_train = np.array(sent_train)\n",
        "sent_test = np.array(sent_test)\n",
        "sent_train = np.nan_to_num(sent_train)\n",
        "sent_test = np.nan_to_num(sent_test)\n"
      ],
      "metadata": {
        "id": "SDYIwurtdGy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLoading GloVe model, this can take some time...\\n...\")\n",
        "embeddings_index = {}\n",
        "f = open('/content/drive/MyDrive/glove.6B.300d.txt', encoding='utf-8')\n",
        "#f = open('word2vec_cdotdata.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "        coefs = np.asarray(values[1:], dtype='float')\n",
        "        embeddings_index[word] = coefs\n",
        "    except ValueError:\n",
        "        continue\n",
        "f.close()\n",
        "print(\"Completed loading pretrained models.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNl-QBWMz627",
        "outputId": "b4f9e24c-ce27-4503-e353-f2c74a74483e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading GloVe model, this can take some time...\n",
            "...\n",
            "Completed loading pretrained models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nTotal %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADrVFqOT0RUM",
        "outputId": "954da704-7eac-4f47-b7b2-e00a0b09c84e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total 59942 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building Hierachical Attention network"
      ],
      "metadata": {
        "id": "pWQ_sGTn03zt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the model and Saving"
      ],
      "metadata": {
        "id": "rk9s5ME03-Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)  # Should be (n_samples, MAX_SENT_LENGTH)\n",
        "print(prof_feat_train.shape)  # Should match input_shape\n",
        "print(cat1_train.shape)  # Should be (n_samples, 20)\n",
        "print(cat2_train.shape)  # Should be (n_samples, 20)\n",
        "print(cat3_train.shape)  # Should be (n_samples, 20)\n",
        "print(y_train.shape)  # Should match number of emotion classes, e.g., (n_samples, 2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNA_JpTp8pEA",
        "outputId": "efaef58a-4a0d-4751-d69e-8cf8c2b98c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8479, 100)\n",
            "(8479, 48, 48, 3)\n",
            "(8479, 20)\n",
            "(8479, 20)\n",
            "(8479, 20)\n",
            "(8479, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iL4fFEVJ9JhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, GRU, Bidirectional, Dense, Dropout, Conv2D, MaxPooling2D, Flatten, concatenate\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import keras.backend as K\n",
        "\n",
        "# Assuming the necessary parameters are already defined\n",
        "MAX_SENT_LENGTH = 100\n",
        "MAX_SENTS = 15\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.1\n",
        "TEST_SPLIT = 0.2\n",
        "\n",
        "# Load or initialize your word embedding matrix\n",
        "embedding_matrix = np.random.random((len(word_index) + 1, 300))  # Replace with actual pre-trained embeddings if needed\n",
        "\n",
        "# Assuming you have a function to calculate Pearson correlation, and a custom attention layer\n",
        "def pearson_cf(y_true, y_pred):\n",
        "    a = y_true - K.mean(y_true)\n",
        "    b = y_pred - K.mean(y_pred)\n",
        "    num = K.sum(a * b)\n",
        "    den = K.sqrt(K.sum(a**2) * K.sum(b**2))\n",
        "    return (num/(den+0.000001))\n",
        "\n",
        "# Define input layers\n",
        "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32', name='sentence_input')\n",
        "descr_input = Input(shape=(48, 48, 3), dtype='float32', name='descr_input')\n",
        "cat1_input = Input(shape=(20,), dtype='int32', name='cat1_input')\n",
        "cat2_input = Input(shape=(20,), dtype='int32', name='cat2_input')\n",
        "cat3_input = Input(shape=(20,), dtype='int32', name='cat3_input')\n",
        "\n",
        "# Embedding layer for text input\n",
        "embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SENT_LENGTH, trainable=False, mask_zero=True)\n",
        "\n",
        "# Embedding layer for categorical inputs\n",
        "embedding_layer1 = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=20, trainable=False, mask_zero=True)\n",
        "\n",
        "# Sentence-level GRU + Attention\n",
        "embedded_sequences = embedding_layer(sentence_input)\n",
        "l_gru = Bidirectional(GRU(128, return_sequences=True))(embedded_sequences)\n",
        "l_att_sent1 = AttLayer(100)(l_gru)  # You will need to implement or define the `AttLayer`\n",
        "l_att_sent1 = Dropout(0.25)(l_att_sent1)\n",
        "l_att_sent1 = Dense(100, activation='relu')(l_att_sent1)\n",
        "l_att_sent1 = Dropout(0.25)(l_att_sent1)\n",
        "\n",
        "# Category-level GRU + Attention for categorical features\n",
        "embedded_sequences_cat1 = embedding_layer1(cat1_input)\n",
        "l_gru1 = Bidirectional(GRU(128, return_sequences=True))(embedded_sequences_cat1)\n",
        "l_att_sent11 = AttLayer(100)(l_gru1)\n",
        "l_att_sent11 = Dropout(0.25)(l_att_sent11)\n",
        "l_att_sent11 = Dense(100, activation='relu')(l_att_sent11)\n",
        "l_att_sent11 = Dropout(0.25)(l_att_sent11)\n",
        "\n",
        "# Repeat for cat2_input and cat3_input\n",
        "embedded_sequences_cat2 = embedding_layer1(cat2_input)\n",
        "l_gru2 = Bidirectional(GRU(128, return_sequences=True))(embedded_sequences_cat2)\n",
        "l_att_sent12 = AttLayer(100)(l_gru2)\n",
        "l_att_sent12 = Dropout(0.25)(l_att_sent12)\n",
        "l_att_sent12 = Dense(100, activation='relu')(l_att_sent12)\n",
        "l_att_sent12 = Dropout(0.25)(l_att_sent12)\n",
        "\n",
        "embedded_sequences_cat3 = embedding_layer1(cat3_input)\n",
        "l_gru3 = Bidirectional(GRU(128, return_sequences=True))(embedded_sequences_cat3)\n",
        "l_att_sent13 = AttLayer(100)(l_gru3)\n",
        "l_att_sent13 = Dropout(0.25)(l_att_sent13)\n",
        "l_att_sent13 = Dense(100, activation='relu')(l_att_sent13)\n",
        "l_att_sent13 = Dropout(0.25)(l_att_sent13)\n",
        "\n",
        "# CNN for image features (prof_feat_train)\n",
        "conv = Conv2D(5, (2, 2), padding='same', activation='relu')(descr_input)\n",
        "conv = MaxPooling2D(pool_size=(3, 3))(conv)\n",
        "conv = Conv2D(5, (2, 2), activation='relu')(conv)\n",
        "conv = MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "conv = Dropout(0.25)(conv)\n",
        "conv = Flatten()(conv)\n",
        "conv = Dense(100, activation='relu')(conv)\n",
        "conv = Dropout(0.25)(conv)\n",
        "\n",
        "# Concatenate all features (text, categorical, image)\n",
        "out_emo1 = Dense(450, activation='relu')(concatenate([l_att_sent1, l_att_sent11, l_att_sent12, l_att_sent13, conv], axis=-1))\n",
        "out_emo1 = Dropout(0.25)(out_emo1)\n",
        "\n",
        "# Output layer (Emotion classification)\n",
        "out_emo = Dense(3, activation='softmax', name='out_emo')(out_emo1)\n",
        "\n",
        "# Model compilation\n",
        "model = Model([sentence_input, descr_input, cat1_input, cat2_input, cat3_input], out_emo)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set learning rate\n",
        "model.optimizer.learning_rate = 1e-3\n",
        "\n",
        "# Define model checkpoint callback\n",
        "weight_file = 'weights/user_desc+image_conv2d+cat.keras'\n",
        "callback = [ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "\n",
        "# Train the model\n",
        "model.fit([x_train, prof_feat_train, cat1_train, cat2_train, cat3_train], y_train, batch_size=64, epochs=15, shuffle=True, validation_split=0.1, verbose=1, callbacks=callback)\n",
        "\n",
        "# Load best weights\n",
        "model.load_weights(weight_file)\n",
        "\n",
        "# Save final model\n",
        "model.save('models/user_desc+image_conv2d+cat.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "iW9IErbH0q7S",
        "outputId": "a7fb1873-2856-4617-a550-5fa2db93b3bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "Exception encountered when calling AttLayer.call().\n\n\u001b[1mmodule 'keras.backend' has no attribute 'tanh'\u001b[0m\n\nArguments received by AttLayer.call():\n  • x=tf.Tensor(shape=(None, 20, 256), dtype=float32)\n  • mask=tf.Tensor(shape=(None, 20), dtype=bool)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-219-6b97601fcadc>\u001b[0m in \u001b[0;36m<cell line: 99>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprof_feat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat2_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat3_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# Load best weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-215-5f11fcac6867>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Compute attention scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0muit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: [batch_size, timesteps, attention_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: [batch_size, timesteps, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: [batch_size, timesteps]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Exception encountered when calling AttLayer.call().\n\n\u001b[1mmodule 'keras.backend' has no attribute 'tanh'\u001b[0m\n\nArguments received by AttLayer.call():\n  • x=tf.Tensor(shape=(None, 20, 256), dtype=float32)\n  • mask=tf.Tensor(shape=(None, 20), dtype=bool)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTl6v4ZjEq7t",
        "outputId": "3e82056b-0c0b-4699-f08a-47179b7c4a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8479, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "xuPdt2d74CaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input shapes based on the given constants\n",
        "num_features_descr = 50   # Set based on your description feature size\n",
        "num_features_cat1 = 10    # Set based on your category 1 feature size\n",
        "num_features_cat2 = 10    # Set based on your category 2 feature size\n",
        "num_features_cat3 = 10    # Set based on your category 3 feature size"
      ],
      "metadata": {
        "id": "hAnFsDne1Ebr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(prof_feat_train.shape)\n",
        "print(cat1_train.shape)\n",
        "print(cat2_train.shape)\n",
        "print(cat3_train.shape)\n",
        "print(y_train.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "zQkK7N5p7Czw",
        "outputId": "cf3991c4-e8b8-4528-b1cd-5027f4bcb4d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8479, 100)\n",
            "(8479, 48, 48, 3)\n",
            "(8479, 20)\n",
            "(8479, 20)\n",
            "(8479, 20)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-188-313d45e9dd3b>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat2_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat3_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train, num_classes=3)  # Adjust num_classes based on your output layer\n"
      ],
      "metadata": {
        "id": "sZSxxGUT8Kp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building Hierachical Attention network\n",
        "embedding_matrix = np.random.random((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "def pearson_cf(y_true, y_pred):\n",
        "    a = y_true - K.mean(y_true)\n",
        "    b = y_pred - K.mean(y_pred)\n",
        "    num = K.sum(a * b)\n",
        "    den = K.sqrt(K.sum(a**2) * K.sum(b**2))\n",
        "    return (num/(den+0.000001))\n",
        "\n",
        "target_names_dep = ['dep', 'non-dep']\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            300,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SENT_LENGTH,\n",
        "                            trainable=False,\n",
        "                            mask_zero=True)\n",
        "\n",
        "embedding_layer1 = Embedding(len(word_index) + 1,\n",
        "                            300,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=20,\n",
        "                            trainable=False,\n",
        "                            mask_zero=True)\n",
        "\n",
        "\n",
        "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
        "descr_input = Input(shape=input_shape, dtype='float32')\n",
        "cat1_input = Input(shape=(20,), dtype='int32')\n",
        "cat2_input = Input(shape=(20,), dtype='int32')\n",
        "cat3_input = Input(shape=(20,), dtype='int32')\n",
        "\n",
        "sent_input = Input(shape=(1,))\n",
        "\n",
        "conv = Conv2D(5, (2, 2), padding='same', activation='relu', input_shape=input_shape)(descr_input)\n",
        "conv = MaxPooling2D(pool_size=(3, 3))(conv)\n",
        "conv = Conv2D(5, (2, 2), activation='relu')(conv)\n",
        "conv = MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "conv = Dropout(0.25)(conv)\n",
        "\n",
        "conv = Flatten()(conv)\n",
        "\n",
        "conv = Dense(100, activation='relu')(conv)\n",
        "conv = Dropout(0.25)(conv)\n",
        "\n",
        "embedded_sequences = embedding_layer(sentence_input)\n",
        "\n",
        "l_gru = Bidirectional(GRU(128, return_sequences=True))(embedded_sequences)\n",
        "l_att_sent1 = AttLayer(100)(l_gru)\n",
        "l_att_sent1 = Dropout(0.25)(l_att_sent1)\n",
        "\n",
        "l_att_sent1 = Dense(100, activation='relu')(l_att_sent1)\n",
        "l_att_sent1 = Dropout(0.25)(l_att_sent1)\n",
        "\n",
        "embedded_sequences_cat1 = embedding_layer1(cat1_input)\n",
        "\n",
        "l_gru1 = Bidirectional(GRU(128, return_sequences=True))(embedded_sequences_cat1)\n",
        "l_att_sent11 = AttLayer(100)(l_gru1)\n",
        "l_att_sent11 = Dropout(0.25)(l_att_sent11)\n",
        "\n",
        "l_att_sent11 = Dense(100, activation='relu')(l_att_sent11)\n",
        "l_att_sent11 = Dropout(0.25)(l_att_sent11)\n",
        "\n",
        "embedded_sequences_cat2 = embedding_layer1(cat2_input)\n",
        "\n",
        "l_gru2 = Bidirectional(GRU(128, return_sequences=True))(embedded_sequences_cat2)\n",
        "l_att_sent12 = AttLayer(100)(l_gru2)\n",
        "l_att_sent12 = Dropout(0.25)(l_att_sent12)\n",
        "\n",
        "l_att_sent12 = Dense(100, activation='relu')(l_att_sent12)\n",
        "l_att_sent12 = Dropout(0.25)(l_att_sent12)\n",
        "\n",
        "embedded_sequences_cat3 = embedding_layer1(cat3_input)\n",
        "\n",
        "l_gru3 = Bidirectional(GRU(128, return_sequences=True))(embedded_sequences_cat3)\n",
        "l_att_sent13 = AttLayer(100)(l_gru3)\n",
        "l_att_sent13 = Dropout(0.25)(l_att_sent13)\n",
        "\n",
        "l_att_sent13 = Dense(100, activation='relu')(l_att_sent13)\n",
        "l_att_sent13 = Dropout(0.25)(l_att_sent13)\n",
        "\n",
        "out_emo1 = Dense(450, activation='relu')(concatenate([l_att_sent1, l_att_sent11, l_att_sent12, l_att_sent13, conv], axis=-1))\n",
        "out_emo1 = Dropout(0.25)(out_emo1)\n",
        "\n",
        "out_emo = Dense(3, activation='softmax', name='out_emo')(out_emo1)\n",
        "\n",
        "model = Model(inputs=[sentence_input, descr_input, cat1_input, cat2_input, cat3_input], outputs=out_emo)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "weight_file = '/content/drive/MyDrive/Weights/user_desc+image_conv2d.keras'\n",
        "callback = [ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "\n",
        "# Change learning rate to 1e-3\n",
        "model.optimizer.learning_rate.assign(1e-3)\n",
        "\n",
        "# Now you can train the model\n",
        "model.fit([x_train, prof_feat_train, cat1_train, cat2_train, cat3_train],\n",
        "          y_train,  # Output labels\n",
        "          batch_size=64,  # Batch size\n",
        "          epochs=15,  # Number of epochs\n",
        "          shuffle=True,  # Shuffle data each epoch\n",
        "          validation_split=0.1,  # Validation split (10%)\n",
        "          verbose=1,  # Verbosity level (1 = progress bar)\n",
        "          callbacks=callback)  # Model checkpoint callback\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WLSLgv94sOo",
        "outputId": "dcd27ca5-7c87-4597-c15f-0ecf3d256dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_855', 'keras_tensor_856', 'keras_tensor_857', 'keras_tensor_858', 'keras_tensor_859']. Received: the structure of inputs=('*', '*', '*', '*', '*')\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 1s/step - accuracy: 0.5874 - loss: 0.7017 - val_accuracy: 0.6592 - val_loss: 0.6093\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_acc available, skipping.\n",
            "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.6729 - loss: 0.6027 - val_accuracy: 0.6757 - val_loss: 0.5832\n",
            "Epoch 3/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 1s/step - accuracy: 0.7009 - loss: 0.5616 - val_accuracy: 0.6698 - val_loss: 0.5842\n",
            "Epoch 4/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.7418 - loss: 0.5182 - val_accuracy: 0.6710 - val_loss: 0.6026\n",
            "Epoch 5/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - accuracy: 0.7698 - loss: 0.4734 - val_accuracy: 0.6828 - val_loss: 0.5813\n",
            "Epoch 6/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.7935 - loss: 0.4335 - val_accuracy: 0.6816 - val_loss: 0.6279\n",
            "Epoch 7/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 1s/step - accuracy: 0.8325 - loss: 0.3770 - val_accuracy: 0.6816 - val_loss: 0.6715\n",
            "Epoch 8/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - accuracy: 0.8542 - loss: 0.3219 - val_accuracy: 0.6910 - val_loss: 0.6804\n",
            "Epoch 9/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - accuracy: 0.8774 - loss: 0.2856 - val_accuracy: 0.6922 - val_loss: 0.7431\n",
            "Epoch 10/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 1s/step - accuracy: 0.9042 - loss: 0.2353 - val_accuracy: 0.6969 - val_loss: 0.8555\n",
            "Epoch 11/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 1s/step - accuracy: 0.9174 - loss: 0.2033 - val_accuracy: 0.7087 - val_loss: 0.9540\n",
            "Epoch 12/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - accuracy: 0.9324 - loss: 0.1654 - val_accuracy: 0.6910 - val_loss: 0.9836\n",
            "Epoch 13/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - accuracy: 0.9449 - loss: 0.1451 - val_accuracy: 0.6887 - val_loss: 1.1705\n",
            "Epoch 14/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 1s/step - accuracy: 0.9550 - loss: 0.1203 - val_accuracy: 0.6851 - val_loss: 1.3767\n",
            "Epoch 15/15\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 1s/step - accuracy: 0.9517 - loss: 0.1323 - val_accuracy: 0.6910 - val_loss: 1.2599\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b80fb34af80>"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        ")\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import seaborn as sns\n",
        "\n",
        "# Predict probabilities\n",
        "a = model.predict([x_test, prof_feat_test])\n",
        "\n",
        "# Get predictions and ground truth\n",
        "pred1_test = np.argmax(a, axis=1)\n",
        "classes_test = np.argmax(to_categorical(y_test), axis=1)[:, 1]\n",
        "predictions1_test = np.array(pred1_test)\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(classes_test, predictions1_test)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
        "    classes_test, predictions1_test, average=\"macro\"\n",
        ")\n",
        "classification_rep = classification_report(\n",
        "    classes_test, predictions1_test, target_names=target_names_dep\n",
        ")\n",
        "conf_matrix = confusion_matrix(classes_test, predictions1_test)\n",
        "\n",
        "# Save Metrics to a File\n",
        "with open(\"/content/drive/MyDrive/Weights/user_desc+image_conv2d.keras\", \"w\") as f:\n",
        "    f.write(f\"Test Accuracy: {accuracy}\\n\")\n",
        "    f.write(f\"Test P_R_F Score: Precision={precision}, Recall={recall}, F1 Score={f1_score}\\n\")\n",
        "    f.write(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n",
        "    f.write(f\"Classification Report:\\n{classification_rep}\\n\")\n",
        "\n",
        "# Print Metrics to Console\n",
        "print(f\"\\nTest Accuracy: {accuracy}\\n\")\n",
        "print(f\"Test P_R_F Score: Precision={precision}, Recall={recall}, F1 Score={f1_score}\\n\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n",
        "print(f\"Classification Report:\\n{classification_rep}\\n\")\n",
        "\n",
        "# Visualization: Heatmap for Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    conf_matrix,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=target_names_dep,\n",
        "    yticklabels=target_names_dep,\n",
        ")\n",
        "plt.title(\"Confusion Matrix Heatmap\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "\n",
        "# Save the heatmap as a PNG file\n",
        "heatmap_file = \"confusion_matrix_heatmap.png\"\n",
        "plt.savefig(heatmap_file, dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nMetrics saved to 'depression_metrics.txt'\")\n",
        "print(f\"Heatmap saved to '{heatmap_file}'\")\n"
      ],
      "metadata": {
        "id": "Dj8_5xYWZ9do",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "outputId": "e5f0a402-67b2-48b1-8111-e5ce76f22aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 186ms/step\n",
            "\n",
            "Test Accuracy: 0.6476415094339623\n",
            "\n",
            "Test P_R_F Score: Precision=0.642107992372286, Recall=0.637834527179953, F1 Score=0.6385069408064361\n",
            "\n",
            "Confusion Matrix:\n",
            "[[855 324]\n",
            " [423 518]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         dep       0.67      0.73      0.70      1179\n",
            "     non-dep       0.62      0.55      0.58       941\n",
            "\n",
            "    accuracy                           0.65      2120\n",
            "   macro avg       0.64      0.64      0.64      2120\n",
            "weighted avg       0.65      0.65      0.64      2120\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics saved to 'depression_metrics.txt'\n",
            "Heatmap saved to 'confusion_matrix_heatmap.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "gK1O4kTj6Tjs",
        "outputId": "f19921af-e6a6-4799-8c8f-2783cd12da79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "I/O operation on closed file.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-961df186eea8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred1_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mB7yOqn77D6T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}